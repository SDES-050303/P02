import streamlit as st
import os
import glob
import re
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.util import ngrams
from nltk.tokenize import RegexpTokenizer

# --- Preparar NLTK ---
nltk.data.path.append('./nltk_data')
nltk.download('stopwords', quiet=True)

# --- Configuraci√≥n Inicial ---
st.set_page_config(page_title="An√°lisis de Letras de Canciones", layout="wide")
DATA_DIR = "Data songs"

# --- Funciones auxiliares ---
def cargar_archivos(carpeta):
    ruta = os.path.join(DATA_DIR, carpeta, "*.txt")
    archivos = glob.glob(ruta)
    nombres = [os.path.basename(archivo) for archivo in archivos]
    return dict(zip(nombres, archivos))

# Selecci√≥n de idioma
idioma = st.selectbox("Selecciona el idioma del an√°lisis", ["espa√±ol", "ingl√©s"])
stop_words = set(stopwords.words('spanish' if idioma == "espa√±ol" else 'english'))

# Usar tokenizer simple sin punkt
tokenizer = RegexpTokenizer(r'\w+')

def limpiar_texto(texto):
    texto = texto.lower()
    tokens = tokenizer.tokenize(texto)
    tokens = [word for word in tokens if word not in stop_words]
    return tokens

def obtener_ngramas(tokens, n):
    return list(ngrams(tokens, n))

def mostrar_wordcloud(tokens):
    texto = ' '.join(tokens)
    wc = WordCloud(width=800, height=400, background_color='white').generate(texto)
    st.image(wc.to_array())

def mostrar_distribucion(tokens):
    frec = Counter(tokens)
    comunes = frec.most_common(20)
    palabras, cantidades = zip(*comunes)
    plt.figure(figsize=(10,5))
    plt.bar(palabras, cantidades)
    plt.xticks(rotation=45)
    st.pyplot(plt)

# --- Interfaz Streamlit ---
st.title("üéµ An√°lisis de Letras de Canciones")

# Selecci√≥n de carpeta
carpetas = [nombre for nombre in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, nombre))]
carpeta_seleccionada = st.selectbox("Selecciona una carpeta (g√©nero/√©poca)", carpetas)

# Cargar archivos de esa carpeta
archivos = cargar_archivos(carpeta_seleccionada)
canciones_seleccionadas = st.multiselect("Selecciona una o m√°s canciones", list(archivos.keys()))

if canciones_seleccionadas:
    for nombre in canciones_seleccionadas:
        st.header(f"üìÑ {nombre}")
        with open(archivos[nombre], 'r', encoding='utf-8') as f:
            texto = f.read()
        
        tokens = limpiar_texto(texto)

        # --- M√©tricas ---
        total_palabras = len(tokens)
        num_oraciones = texto.count('.') + texto.count('!') + texto.count('?')
        promedio = total_palabras / max(1, num_oraciones)
        palabras_unicas = set(tokens)
        frec = Counter(tokens)

        col1, col2, col3 = st.columns(3)
        col1.metric("Total Palabras", total_palabras)
        col2.metric("Promedio por oraci√≥n", f"{promedio:.2f}")
        col3.metric("Palabras √önicas", len(palabras_unicas))

        # --- Palabras Comunes ---
        st.subheader("üîù Palabras m√°s comunes")
        comunes = frec.most_common(10)
        for palabra, freq in comunes:
            st.write(f"{palabra}: {freq}")

        # --- WordCloud ---
        st.subheader("‚òÅÔ∏è WordCloud")
        mostrar_wordcloud(tokens)

        # --- Distribuci√≥n ---
        st.subheader("üìä Distribuci√≥n de Vocabulario")
        mostrar_distribucion(tokens)

        # --- N-gramas ---
        st.subheader("üìé N-Gramas")
        for n in [2, 3, 4]:
            ngramas = obtener_ngramas(tokens, n)
            frec_ng = Counter(ngramas).most_common(5)
            st.markdown(f"**Top {n}-gramas:**")
            for ng, freq in frec_ng:
                st.write(f"{' '.join(ng)}: {freq}")
